# -*- coding: utf-8 -*-
"""one.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C4Xg_43JPuEo_Zdyvha8GZ5tj5R8GI1Q
"""

import numpy as np
import pandas as pd
import math
from scipy import stats
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

x=pd.read_csv("/content/crop data.csv")

x.head(10)

x.tail(20)

x.info()

x.describe()

x.columns

x['label'].unique()

count=x['label'].value_counts()

x.isnull().sum()

x.loc[x.duplicated()]

corr = x.corr()
corr

sns.heatmap(corr,annot=True,cbar=True,cmap='coolwarm')

col=['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']

for column in col:
    plt.figure()
    sns.histplot(x[column], kde=True)
    plt.title(f'Histogram of {column}')
    plt.xlabel('Values')
    plt.ylabel('Frequency')

plt.show()

x.shape

column_to_drop=['label']

x['label'], label_mapping = pd.factorize(x['label'])


print(x)

print("Label Mapping:")
print(dict(zip(label_mapping, range(len(label_mapping)))))

X = x.drop(columns=column_to_drop, axis=1).values
y = x['label'].values

X

X.shape

y

class DecisionTree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = None

    def fit(self, X, y):
        self.tree = self._grow_tree(X, y, depth=0)

    def _grow_tree(self, X, y, depth):
        num_samples, num_features = X.shape
        unique_classes = np.unique(y)

        if len(unique_classes) == 1 or (self.max_depth is not None and depth == self.max_depth):
            return {'class': unique_classes[0], 'count': len(y)}


        if num_features == 0:
            class_counts = [len(y[y == cls]) for cls in unique_classes]
            return {'class': unique_classes[np.argmax(class_counts)], 'count': len(y)}


        feature_index, threshold = self._find_best_split(X, y)

        if feature_index is None:
            class_counts = [len(y[y == cls]) for cls in unique_classes]
            return {'class': unique_classes[np.argmax(class_counts)], 'count': len(y)}

        indices_left = X[:, feature_index] <= threshold
        X_left, y_left = X[indices_left], y[indices_left]
        X_right, y_right = X[~indices_left], y[~indices_left]

        left_tree = self._grow_tree(X_left, y_left, depth + 1)
        right_tree = self._grow_tree(X_right, y_right, depth + 1)

        return {'feature_index': feature_index, 'threshold': threshold,
                'left': left_tree, 'right': right_tree}

    def _find_best_split(self, X, y):
        num_samples, num_features = X.shape
        if num_samples <= 1:
            return None, None

        gini = np.inf
        for feature_index in range(num_features):
            thresholds = np.unique(X[:, feature_index])
            for threshold in thresholds:
                indices_left = X[:, feature_index] <= threshold
                indices_right = ~indices_left
                gini_left = self._calculate_gini(y[indices_left])
                gini_right = self._calculate_gini(y[indices_right])
                weighted_gini = (len(y[indices_left]) / num_samples) * gini_left + \
                                (len(y[indices_right]) / num_samples) * gini_right

                if weighted_gini < gini:
                    gini = weighted_gini
                    best_feature, best_threshold = feature_index, threshold

        return best_feature, best_threshold

    def _calculate_gini(self, y):
        classes, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        gini = 1 - np.sum(probabilities**2)
        return gini

    def predict(self, X):
        return np.array([self._predict_tree(x, self.tree) for x in X])

    def _predict_tree(self, x, node):
        if 'class' in node:
            return node['class']
        if x[node['feature_index']] <= node['threshold']:
            return self._predict_tree(x, node['left'])
        else:
            return self._predict_tree(x, node['right'])


class RandomForestClassifier:
    def __init__(self, n_estimators=100, max_depth=None):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.estimators = []

    def fit(self, X, y):
        for _ in range(self.n_estimators):
            indices = np.random.choice(len(X), len(X), replace=True)
            X_bootstrap, y_bootstrap = X[indices], y[indices]

            tree = DecisionTree(max_depth=self.max_depth)
            tree.fit(X_bootstrap, y_bootstrap)
            self.estimators.append(tree)

    def predict(self, X):
        predictions = np.array([tree.predict(X) for tree in self.estimators])
        return np.array([np.bincount(predictions[:, i]).argmax() for i in range(predictions.shape[1])])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=30, max_depth=10)

rf_classifier.fit(X_train, y_train)

y_pred = rf_classifier.predict(X_test)

accuracy = accuracy_score(y_test,y_pred )
print(f"Accuracy: {accuracy * 100:.2f}%")

train_predictions = rf_classifier.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)
print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
test_predictions = rf_classifier.predict(X_test)
test_accuracy = accuracy_score(y_test, test_predictions)
print(f"Testing Accuracy: {test_accuracy * 100:.2f}%")

accuracy = accuracy_score(y_test, y_pred)


precision = precision_score(y_test, y_pred, average='weighted')


recall = recall_score(y_test, y_pred, average='weighted')


f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

class_labels = sorted(set(y_test))
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

def get_user_input():
    print("Enter details for the new data point:")
    N = int(input("Enter the value of N: "))
    P = int(input("Enter the value of P: "))
    K = int(input("Enter the value of K: "))
    temperature = float(input("Enter the value of temperature (in Celsius): "))
    humidity = float(input("Enter the value of humidity: "))
    ph = float(input("Enter the pH value: "))
    rainfall = float(input("Enter the amount of rainfall: "))

    user_data = np.array([N, P, K, temperature, humidity, ph, rainfall])

    return user_data

new_data_point = get_user_input()

crop_prediction = rf_classifier.predict(new_data_point.reshape(1, -1))[0]
crop_prediction

crop_label =crop_prediction

label_to_crop = {
    0:'muskmelon',
    1:'watermelon',
    2:'papaya',
    3:'apple',
    4:'mango',
    5:'mothbeans',
    6:'mugbean',
    7:'lentil',
    8:'blackgram',
    9:'coconut',
    10:'pomegranate',
    11:'jute',
    12:'maize',
    13:'coffee',
    14:'orange',
    15:'chickpea',
    16:'peigonpeas',
    17:'rice',
    18:'kidneybeans',
    19:'grapes',
    20:'cotton',
    21:'banana'

}

predicted_crop = label_to_crop.get(crop_label)
print(f"The predicted crop is: {predicted_crop}")